---
title: "Image Outlier Detection: Mahalanobis Distance"
categories:
  - Machine learning
tags:
  - Outlier detection
  - Deep learning
  - Image analysis
header:
  image: &image "https://miro.medium.com/max/2400/1*F_yiILIE954AZPgPADx76A.png"
  caption: ""
  teaser: *image
link: 
classes: wide
# toc: true
# toc_label: "Table of Contents"
# toc_icon: "cog"
---

In this post, I will show how _Mahalanobis distance_ can be used to detect outliers in images. 
We particularly interest to identify those samples which are out of distribution and potentially can be extreme case samples for developed AI models. 


## Mahalanobis Distance
Assume we have a $N$ dimensional data space where $N$ basically indicates number of variables. Then each sample in this space is a point and represented by vector $x=(x_1, x_2, ..., x_N)^T$.
The Mahalanobis distance is a measure of the distance between a point $x$ and the distribution of data.

The Mahalanobis distance for a vector  with a mean vector $\mu$ and covariance matrix $V$ is defined by

$$
D(x) = \sqrt{ (x - \mu)^T V^{-1} (x - \mu) }
$$


