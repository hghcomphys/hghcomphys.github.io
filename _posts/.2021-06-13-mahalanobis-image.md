---
title: "Image Outlier Detection: Mahalanobis Distance"
categories:
  - Machine learning
tags:
  - Outlier detection
  - Deep learning
  - Image analysis
header:
  image: &image "/assets/outlier-detection/images/outlier_mnist.png"
  caption: ""
  teaser: *image
link: 
classes: wide
toc: true
toc_label: "Table of Contents"
toc_icon: "cog"
author_profile: true
# layout: splash
---

In this post, I will explain how _Mahalanobis distance_ can be used to detect image outliers from  [MNIST](http://yann.lecun.com/exdb/mnist/) handwriting digits dataset. 
We particularly interest to identify _known_ (MNIST) or _unknown_ (i.e. Fashion MNIST and CIFAR10 datasets) samples that are out-of-distribution and potentially can be extreme cases for a deployed artificial intelligence model. 


<!-- # Background -->

### Importance of outlier detection
Artificial intelligence (AI) models and particularly deep neural networks (NNs) are powerful algorithms that have recently achieve impressive accuracies in various domains as wide as bioinformatics, disease diagnosis, waste management, and material novel material discovery. 
However, due to their intrinsic black box architecture, NNs exhibit an inclination to produce overconfident predictions and perform poorly on quantifying uncertainty. 
If an AI algorithm makes overconfident incorrect predictions then the consequences can be harmful or even 
catastrophic, for instance in the context of healthcare. 

In statistics, outliers are data points that donâ€™t belong to a certain population.
This is exactly where outlier detection (OD) becomes crucial for practical applications by helping us to identify abnormal samples that are drawn far away from the distribution of the training samples and avoiding potentially wrong predictions.

**Note:** Here we are interested in detecting out-of-distribution samples but those are not the only types of outliers. 
More about different types of outliers can be found [here](https://towardsdatascience.com/outliers-analysis-a-quick-guide-to-the-different-types-of-outliers-e41de37e6bf6).
{: .notice--info}

Outlier detection is in addition used to assess and improve the robustness of an AI model on out-of-distribution samples in the labs which result in increased generalization of developed AI models after deployment.  


### Mahalanobis distance
Assume having a $N$ dimensional multivariate space that $N$ basically indicates the number of variables. Each sample in this space is represented by a point and it is a vector $x=(x_1, x_2, ..., x_N)^T$.
The Mahalanobis distance $D$ of a point $x$ is defined as the relative distance to the centroid of data distribution and it is given by 

$$
D(x) = \sqrt{ (x - \mu)^T V^{-1} (x - \mu) }, 
$$ 

where $\mu$ and $V$ are the mean and covariance matrix of the sample distribution, respectively.
The larger $D(x)$, the further away from the centroid the data point $x$ is.


### Feature space
<!-- Detecting outliers particularly in images is a difficult task. -->
Mahalanobis distance (MD) analysis can be directly applied to any given dataset.
However, for image data, it would be better to use it on the feature space encoded by a neural network which any sample in a feature vector intrinsically describing the appearance and relevant patterns for the labels present in the sample. 

The figure blow shows a schematic representation of a neural network which encodes an input images to feature space.\
<img src="/assets/outlier-detection/images/feature_model.png" alt="drawing" width=500>


## Code description

### Data preparation

#### Creating image data generator
Here we scale image data between `[-1.0, 1.0]` and creating an image data generator from the input _numpy_ array.
The data generator is not necessary here as the size of studied datasets is small (few hundreds of megabytes). 
However, in the real-world problem, we often work with big data which can not be fitted to the memory at once, and in that case, the image data generator is a great tool that helps to deal with images for different tasks such as preprocessing, augmentation, training, and validation.
```python
def scale(img):
    """
    Scale input image between [-1.0, 1.0].
    """
    return img.astype(np.float32)/127.5 - 1.0

def unscale(img):
    """
    Unscale input image between [0, 255].
    """
    return ((img+1.0)*127.5).astype(np.uint8)

def preprocess_image(img):
    """
    Preprocess function for image data generator.
    """
    img = scale(img)
    return img

def create_datagen(X, y, aug=False, onehot=False):
    """
    Create an image data generator from input images.
    """
    # Apply augmentation
    if aug:
        datagen = tf.keras.preprocessing.image.ImageDataGenerator(
            rescale=None,
            preprocessing_function=preprocess_image,
            #vertical_flip=True,
            #horizontal_flip=True, 
            shear_range=0.1,
            zoom_range=0.1,
            height_shift_range=0.1,
            width_shift_range=0.1,
            rotation_range=10,
            brightness_range=[0.7, 1.3],
            fill_mode= "nearest",
        )   
    else:
        datagen = tf.keras.preprocessing.image.ImageDataGenerator(
            rescale=None,
            preprocessing_function=preprocess_image,
        )
    
    # One-hot encodeing 
    labels = y if not onehot else tf.keras.utils.to_categorical(y)
    
    # Create data generator from numpy arrays (X, y).
    batches = datagen.flow(X, labels, batch_size=CFG['batch_size'], shuffle=True)
    batches.samples = X.shape[0]
        
    return batches
```

#### Datasets
The utility function `get_dataset()` helps to easily load different datasets for feature model training and outlier detector validations.
```python
  def get_dataset(name="mnist"):
    """
    Return the entire dataset as a tuple (X, y).
    We need it in order to explicitly define the outliers group.
    """
    if name=="mnist":
        (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
    elif name=="fashion_mnist":
        (X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()
    elif name=="cifar10":
        (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()
    else:
        raise ValueError(f"Unknown dataset {name}")

    X = np.concatenate((X_train, X_test), axis=0)
    y = np.concatenate((y_train, y_test), axis=0)
    
    # Add channel dimension for gray images
    if X.ndim < 4:
        X = X[..., np.newaxis]
        y = y[..., np.newaxis]
    
    return X, y
```


### Feature model
Here, we are using a few convolutions layers and a dense layer on top to build our feature model. 
Classifier includes one extra dense layer with `softmax` activation which is used to train the model based on provided image labels. 
The idea is that we train the entire model as a classifier and then using the same weights for the feature model. This way, all the relevant features from training samples are learned by our AI model which can be used for outlier detection later. 
This helps to have a better perf 
#### Build neural network
```python
def build_custom_model():
    """
    Build a convolutional classifier that is used as the image features extractor.
    """
    conv_regulizer = tf.keras.regularizers.l2(CFG['conv_regularization_factor'])
    dense_regulizer = tf.keras.regularizers.l2(CFG['dense_regularization_factor'])
    # Convolutional layers
    inp = layers.Input(shape=(CFG['image_size'], CFG['image_size'], CFG['channel_size']))
    x = inp
    for filters in CFG['layer_filters']:
        x = layers.Conv2D(filters, CFG['kernel_size'], padding='same', activation='relu', kernel_regularizer=conv_regulizer)(x)
        x = layers.AveragePooling2D((2, 2), padding='same')(x)
    # Top dense layers  
    x = layers.Flatten()(x)
    x= layers.Dropout(0.2)(x)
    features = layers.Dense(64, activation='sigmoid', kernel_regularizer=dense_regulizer)(x)   
    out = layers.Dense(num_labels, activation='softmax', kernel_regularizer=dense_regulizer)(features)
    # Return classifier, features extractor
    return Model(inp, out), Model(inp, features)
```

#### Fit feature model
Defining callbacks to save the best model and improve training. 
```python
# Save model checkpoint
save_dir = os.path.join(os.getcwd(), 'saved_models')
model_name = 'best-model.h5' #'model-checkpoint.{epoch:03d}.h5'
checkpoint_fn  = os.path.join(save_dir, model_name)
csvlog_fn = os.path.join(save_dir, "training.log")
if not os.path.isdir(save_dir):
    os.makedirs(save_dir)
    
# Prepare callbacks for model saving 
checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_fn, monitor='val_loss', verbose=1, save_best_only=True)

# CSV logging callbacks
csv_logger = tf.keras.callbacks.CSVLogger(csvlog_fn, separator=",", append=False)

# Early stop
earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode="min", patience=6, verbose=1)

# Reduced learning rate
reducelr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)

# List of callbacks
callbacks = [csv_logger, earlystop, reducelr, checkpoint]

# Compiler classifier
clf, fm = build_custom_model()
clf.compile(optimizer="adam", loss='categorical_crossentropy', metrics=['accuracy'])
```

Now it is time to train the model. Please note that we also applying `image augmentation` in order to avoid model overfitting.
```python
# Create training and validation image data generators
train_batches= create_datagen(X_train, y_train, aug=True, onehot=True)
val_batches = create_datagen(X_test, y_test, onehot=True)

clf.fit(
    train_batches,
    steps_per_epoch=train_batches.samples//CFG['batch_size'],
    validation_data=val_batches,
    validation_steps=val_batches.samples//CFG['batch_size'],
    epochs=CFG['epochs'],
    callbacks=callbacks,
    workers=8,
    verbose=1,
 )
```

### Mahalanobis outlier detector
If everything goes well, we should have a well-trained feature model now which extracts important features from the input image. To proceed, we define a custom Mahalanobis outlier detector that accepts the feature model. 
```python
class MahalanobisOutlierDetector:
    """
    An outlier detector which uses an input trained model as feature extractor and
    calculates the Mahalanobis distance as an outlier score.
    """
    def __init__(self, features_extractor: Model):
        self.features_extractor = features_extractor
        self.features = None
        self.features_mean = None
        self.features_covmat = None
        self.features_covmat_inv = None
        self.threshold = None
        
    def _extract_features(self, gen, steps, verbose) -> np.ndarray:
        """
        Extract features from the base model.
        """
        if steps is None:
            steps = gen.samples//CFG['batch_size']
        return self.features_extractor.predict(gen, steps=steps, workers=8, verbose=1)
        
    def _init_calculations(self):
        """
        Calculate the prerequired matrices for Mahalanobis distance calculation.
        """
        self.features_mean = np.mean(self.features, axis=0)
        self.features_covmat = np.cov(self.features, rowvar=False)
        self.features_covmat_inv = scipy.linalg.inv(self.features_covmat)
        
    def _calculate_distance(self, x) -> float:
        """
        Calculate Mahalanobis distance for an input instance.
        """
        return scipy.spatial.distance.mahalanobis(x, self.features_mean, self.features_covmat_inv)
    
    def _infer_threshold(self, verbose):
        """
        Infer threshold based on the extracted features from the training set.
        """
        scores = np.asarray([self._calculate_distance(feature) for feature in self.features])
        mean = np.mean(scores)
        std = np.std(scores)
        self.threshold = mean + 2 * std
        if verbose > 0:
            print("OD score mean:", mean)
            print("OD score std :", std)
            print("OD threshold :", self.threshold)  
            
    def fit(self, gen, steps=None, verbose=1):
        """
        Fit detector model.
        """
        self.features = self._extract_features(gen, steps, verbose)
        self._init_calculations()
        self._infer_threshold(verbose)
        
    def predict(self, gen, steps=None, verbose=1) -> np.ndarray:
        """
        Calculate outlier score (Mahalanobis distance).
        """
        features  =  self._extract_features(gen, steps, verbose)
        scores = np.asarray([self._calculate_distance(feature) for feature in features])
        if verbose > 0:
            print("OD score mean:", np.mean(scores))
            print("OD score std :", np.std(scores))
            print(f"Outliers     :{len(np.where(scores > self.threshold )[0])/len(scores): 1.2%}")
            
        if verbose > 1:
            # CFD
            #plt.hist(scores, cumulative=True, density=1, bins=100);
            #plt.axvline(self.threshold, c='k', ls='--', label='threshold')
            #plt.xlabel("Mahalanobis distance"); plt.ylabel("CFD");
            #plt.show()
            # Hist
            plt.hist(scores, bins=100);
            plt.axvline(self.threshold, c='k', ls='--', label='threshold')
            plt.xlabel("Mahalanobis distance"); plt.ylabel("Distribution");
            plt.show()
            
        return scores
```

After calling the `fit` method on training batches which basically calculate the covariance matrix, mean, and hard-core threshold. 
```python
# Create and fit Mahalanobis outlier detector
od = MahalanobisOutlierDetector(features_extractor=fm)
od.fit(train_batches, steps=None, verbose=1)
```

Finally, It is ready to `predict` Mahalanobis distances as outlier scores for any given input batches in form of the image data generator.

```python
od_scores =  od.predict(val_batches, verbose=2)
```

```python
outlier_batches = create_datagen(X_outliers, y_outliers)
od_scores = od.predict(outlier_batches, verbose=2)
``` 


## Results
Here we see the mean of Mahalanobis distance for different populations including MNIST (training), MNIST(validation), MNIST (outlier), Fashion MNIST, and CIFAR10.
As we expect the mean distance for three later datasets is larger than those of which used for training the feature model.

<img src="/assets/outlier-detection/images/md_score_barplot.png" alt="drawing" width=600>

<!--  -->
A figure mixture of box and violin plots of evaluated distances shows how the scores are distributed for different populations.

<img src="/assets/outlier-detection/images/md_score_boxplot.png" alt="drawing" width=600>

<!--  -->
Histogram plot shows the shift in data distribution particularly for the Fashion MNIST dataset

<img src="/assets/outlier-detection/images/md_score_histplot.png" alt="drawing" width=600>


## References
https://arxiv.org/abs/1612.01474