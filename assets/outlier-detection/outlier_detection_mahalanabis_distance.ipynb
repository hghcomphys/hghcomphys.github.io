{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Outlier Detection: Mahalanobis Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Confguration\n",
    "CFG = {\n",
    "    'gpus': 0,\n",
    "    'image_size': 28,\n",
    "    'channel_size': 1,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 5,\n",
    "    'learning_rate': 0.001,\n",
    "    'conv_regularization_factor': 0.0001,\n",
    "    'dense_regularization_factor': 0.0001,\n",
    "    'kernel_size': 3,\n",
    "    'layer_filters': [32, 64, 128],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import types\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Model\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(img):\n",
    "    \"\"\"\n",
    "    Scale input image between [-1.0, 1.0].\n",
    "    \"\"\"\n",
    "    return img.astype(np.float32)/127.5 - 1.0\n",
    "\n",
    "def unscale(img):\n",
    "    \"\"\"\n",
    "    Unscale input image between [0, 255].\n",
    "    \"\"\"\n",
    "    return ((img+1.0)*127.5).astype(np.uint8)\n",
    "\n",
    "def preprocess_image(img):\n",
    "    \"\"\"\n",
    "    Preprocess function for image data generator.\n",
    "    \"\"\"\n",
    "    img = scale(img)\n",
    "    return img\n",
    "\n",
    "def create_datagen(X, y, aug=False, onehot=False):\n",
    "    \"\"\"\n",
    "    Create an image data generator from input images.\n",
    "    \"\"\"\n",
    "    # Apply augmentation\n",
    "    if aug:\n",
    "        datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            rescale=None,\n",
    "            preprocessing_function=preprocess_image,\n",
    "            #vertical_flip=True,\n",
    "            #horizontal_flip=True, \n",
    "            shear_range=0.1,\n",
    "            zoom_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            width_shift_range=0.1,\n",
    "            rotation_range=10,\n",
    "            brightness_range=[0.7, 1.3],\n",
    "            fill_mode= \"nearest\",\n",
    "        )   \n",
    "    else:\n",
    "        datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            rescale=None,\n",
    "            preprocessing_function=preprocess_image,\n",
    "        )\n",
    "    \n",
    "    # One-hot encodeing \n",
    "    labels = y if not onehot else tf.keras.utils.to_categorical(y)\n",
    "    \n",
    "    # Create data generator from numpy arrays (X, y).\n",
    "    batches = datagen.flow(X, labels, batch_size=CFG['batch_size'], shuffle=True)\n",
    "    batches.samples = X.shape[0]\n",
    "        \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(gen, figs_per_row=8, show_one_line=True):\n",
    "    \"\"\"\n",
    "    Show images from a batch of image generator.\n",
    "    \"\"\"\n",
    "    stats = defaultdict(int)\n",
    "    x, y = gen.next()\n",
    "    y=np.squeeze(y)\n",
    "    rows, columns = math.ceil(CFG['batch_size']/figs_per_row), figs_per_row\n",
    "    if show_one_line:\n",
    "        rows = 1\n",
    "    fig=plt.figure(figsize=(columns*1.5, rows*1.5))\n",
    "    try:\n",
    "        for i in range(columns*rows):\n",
    "            img, target = x[i, ...], y[i]\n",
    "            fig.add_subplot(rows, columns, i+1)\n",
    "            plt.title(f\"{target}\")\n",
    "            if x.shape[3]==1:\n",
    "                plt.imshow(unscale(img), cmap=\"gray\")\n",
    "            else:\n",
    "                plt.imshow(unscale(img))\n",
    "            plt.axis('off')\n",
    "    except IndexError:\n",
    "        pass\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data: trainings, tests, and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(name=\"mnist\"):\n",
    "    \"\"\"\n",
    "    Return entire dataset as a tuple (X, y).\n",
    "    We need it to define explicitly outliers group.\n",
    "    \"\"\"\n",
    "    if name==\"mnist\":\n",
    "        (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    elif name==\"fashion_mnist\":\n",
    "        (X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "    elif name==\"cifar10\":\n",
    "        (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset {name}\")\n",
    "\n",
    "    X = np.concatenate((X_train, X_test), axis=0)\n",
    "    y = np.concatenate((y_train, y_test), axis=0)\n",
    "    \n",
    "    # Add channel dimension for gray images\n",
    "    if X.ndim < 4:\n",
    "        X = X[..., np.newaxis]\n",
    "        y = y[..., np.newaxis]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_dataset()\n",
    "\n",
    "# Set some of categories as outliers and the rest for traning classifier\n",
    "outlier_indices = np.squeeze( (y==9) | (y==8) )\n",
    "X_outliers, y_outliers = X[outlier_indices], y[outlier_indices]\n",
    "X_inliers, y_inliers = X[~outlier_indices], y[~outlier_indices]\n",
    "\n",
    "# Split train test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_inliers, y_inliers, test_size=0.20, random_state=2021)\n",
    "num_labels = len(np.unique(y_train))\n",
    "\n",
    "print(\"Number of unique labels:\", num_labels)\n",
    "print(f\"Train   : {X_train.shape}\")\n",
    "print(f\"Test    : {X_test.shape}\")\n",
    "print(f\"Outliers: {X_outliers.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(create_datagen(X_train, y_train, aug=True), show_one_line=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(create_datagen(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(create_datagen(X_outliers, y_outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Features extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_custom_model():\n",
    "    \"\"\"\n",
    "    Build a convolutional classifier which is used as image features extractor.\n",
    "    \"\"\"\n",
    "    conv_regulizer = tf.keras.regularizers.l2(CFG['conv_regularization_factor'])\n",
    "    dense_regulizer = tf.keras.regularizers.l2(CFG['dense_regularization_factor'])\n",
    "    # Convolutional layers\n",
    "    inp = layers.Input(shape=(CFG['image_size'], CFG['image_size'], CFG['channel_size']))\n",
    "    x = inp\n",
    "    for filters in CFG['layer_filters']:\n",
    "        x = layers.Conv2D(filters, CFG['kernel_size'], padding='same', activation='relu', kernel_regularizer=conv_regulizer)(x)\n",
    "        x = layers.AveragePooling2D((2, 2), padding='same')(x)\n",
    "    # Top dense layers  \n",
    "    x = layers.Flatten()(x)\n",
    "    x= layers.Dropout(0.2)(x)\n",
    "    features = layers.Dense(64, activation='sigmoid', kernel_regularizer=dense_regulizer)(x)   \n",
    "    out = layers.Dense(num_labels, activation='softmax', kernel_regularizer=dense_regulizer)(features)\n",
    "    # Return classifier, features extractor\n",
    "    return Model(inp, out), Model(inp, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model checkpoint\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'best-model.h5' #'model-checkpoint.{epoch:03d}.h5'\n",
    "checkpoint_fn  = os.path.join(save_dir, model_name)\n",
    "csvlog_fn = os.path.join(save_dir, \"training.log\")\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "# Prepare callbacks for model saving \n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_fn, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "# CSV logging callbacks\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(csvlog_fn, separator=\",\", append=False)\n",
    "\n",
    "# Early stop\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode=\"min\", patience=6, verbose=1)\n",
    "\n",
    "# Reduced learning rate\n",
    "reducelr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
    "\n",
    "# List of callbacks\n",
    "callbacks = [csv_logger, earlystop, reducelr, checkpoint]\n",
    "\n",
    "# Compiler classifier\n",
    "clf, fm = build_custom_model()\n",
    "clf.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classfier \n",
    "clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features extractor as model \n",
    "fm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation image data generators\n",
    "train_batches= create_datagen(X_train, y_train, aug=True, onehot=True)\n",
    "val_batches = create_datagen(X_test, y_test, onehot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(\n",
    "    train_batches,\n",
    "    steps_per_epoch=train_batches.samples//CFG['batch_size'],\n",
    "    validation_data=val_batches,\n",
    "    validation_steps=val_batches.samples//CFG['batch_size'],\n",
    "    epochs=CFG['epochs'],\n",
    "    callbacks=callbacks,\n",
    "    workers=8,\n",
    "    verbose=1,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pd.read_csv(csvlog_fn)\n",
    "log.plot(x=\"epoch\", y=[\"loss\", \"val_loss\"])\n",
    "# log.plot(x=\"epoch\", y=[\"accuracy\", \"val_accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mahalanobis outlier detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MahalanobisOutlierDetector:\n",
    "    \"\"\"\n",
    "    An outlier detector which uses an input trained model as feature extractor and\n",
    "    calculates the Mahalanobis distance as outlier score.\n",
    "    \"\"\"\n",
    "    def __init__(self, features_extractor: Model):\n",
    "        self.features_extractor = features_extractor\n",
    "        self.features = None\n",
    "        self.features_mean = None\n",
    "        self.features_covmat = None\n",
    "        self.features_covmat_inv = None\n",
    "        self.threshold = None\n",
    "        \n",
    "    def _extract_features(self, gen, steps, verbose) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Extract features from the base model.\n",
    "        \"\"\"\n",
    "        if steps is None:\n",
    "            steps = gen.samples//CFG['batch_size']\n",
    "        return self.features_extractor.predict(gen, steps=steps, workers=8, verbose=1)\n",
    "        \n",
    "    def _init_calculations(self):\n",
    "        \"\"\"\n",
    "        Calculate the prerequired matrices for Mahalanobis distance calculation.\n",
    "        \"\"\"\n",
    "        self.features_mean = np.mean(self.features, axis=0)\n",
    "        self.features_covmat = np.cov(self.features, rowvar=False)\n",
    "        self.features_covmat_inv = scipy.linalg.inv(self.features_covmat)\n",
    "        \n",
    "    def _calculate_distance(self, x) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Mahalanobis distance for an input instance.\n",
    "        \"\"\"\n",
    "        return scipy.spatial.distance.mahalanobis(x, self.features_mean, self.features_covmat_inv)\n",
    "    \n",
    "    def _infer_threshold(self, verbose):\n",
    "        \"\"\"\n",
    "        Infer threshold based on the extracted featres from the training set.\n",
    "        \"\"\"\n",
    "        scores = np.asarray([self._calculate_distance(feature) for feature in self.features])\n",
    "        mean = np.mean(scores)\n",
    "        std = np.std(scores)\n",
    "        self.threshold = mean + 2 * std\n",
    "        if verbose > 0:\n",
    "            print(\"OD score mean:\", mean)\n",
    "            print(\"OD score std :\", std)\n",
    "            print(\"OD threshold :\", self.threshold)  \n",
    "            \n",
    "    def fit(self, gen, steps=None, verbose=1):\n",
    "        \"\"\"\n",
    "        Fit detector model.\n",
    "        \"\"\"\n",
    "        self.features = self._extract_features(gen, steps, verbose)\n",
    "        self._init_calculations()\n",
    "        self._infer_threshold(verbose)\n",
    "        \n",
    "    def predict(self, gen, steps=None, verbose=1) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate outlier score (Mahalanobis distance).\n",
    "        \"\"\"\n",
    "        features  =  self._extract_features(gen, steps, verbose)\n",
    "        scores = np.asarray([self._calculate_distance(feature) for feature in features])\n",
    "        if verbose > 0:\n",
    "            print(\"OD score mean:\", np.mean(scores))\n",
    "            print(\"OD score std :\", np.std(scores))\n",
    "            print(f\"Outliers     :{len(np.where(scores > self.threshold )[0])/len(scores): 1.2%}\")\n",
    "            \n",
    "        if verbose > 1:\n",
    "            # CFD\n",
    "            #plt.hist(scores, cumulative=True, density=1, bins=100);\n",
    "            #plt.axvline(self.threshold, c='k', ls='--', label='threshold')\n",
    "            #plt.xlabel(\"Mahalanobis distance\"); plt.ylabel(\"CFD\");\n",
    "            #plt.show()\n",
    "            # Hist\n",
    "            plt.hist(scores, bins=100);\n",
    "            plt.axvline(self.threshold, c='k', ls='--', label='threshold')\n",
    "            plt.xlabel(\"Mahalanobis distance\"); plt.ylabel(\"Distribution\");\n",
    "            plt.show()\n",
    "            \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and fit Mahalanobis outlier detector\n",
    "od = MahalanobisOutlierDetector(features_extractor=fm)\n",
    "od.fit(train_batches, steps=None, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_results = defaultdict(dict)\n",
    "train_batches = create_datagen(X_train, y_train, aug=False, onehot=True)\n",
    "od_results[\"MNIST (training)\"][\"od_scores\"] = od.predict(train_batches, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_results[\"MNIST (validation)\"][\"od_scores\"] =  od.predict(val_batches, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_batches = create_datagen(X_outliers, y_outliers)\n",
    "od_results[\"MNIST (outliers)\"][\"od_scores\"] = od.predict(outlier_batches, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_dataset(\"fashion_mnist\")\n",
    "fashion_batches = create_datagen(X, y)\n",
    "show_batch(fashion_batches)\n",
    "od_results[\"Fashion MNIST\"][\"od_scores\"] = od.predict(fashion_batches, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_dataset(\"cifar10\")\n",
    "X_ = np.asarray([(255*rgb2gray(img[2:-2, 2:-2])[..., np.newaxis]).astype(np.uint8) for img in X])\n",
    "cifar10_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=None, preprocessing_function=preprocess_image)\n",
    "cifar10_batches = cifar10_datagen.flow(X_, y, batch_size=CFG['batch_size'], shuffle=True)\n",
    "cifar10_batches.samples = len(X_)\n",
    "show_batch(cifar10_batches)\n",
    "od_results[\"CIFAR10\"][\"od_scores\"] = od.predict(cifar10_batches, verbose=2)\n",
    "\n",
    "X, y = next(cifar10_batches)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_results_df = pd.DataFrame(od_results).transpose()\n",
    "od_results_df[\"n_samples\"] = od_results_df[\"od_scores\"].map(lambda x: len(x))\n",
    "od_results_df[\"od_score_mean\"] = od_results_df[\"od_scores\"].map(lambda x: np.mean(x))\n",
    "od_results_df[\"od_score_std\"] = od_results_df[\"od_scores\"].map(lambda x: np.std(x))\n",
    "od_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "# sns.barplot(x=od_results_df.index, y=od_results_df[\"n_samples\"])\n",
    "sns.barplot(x=od_results_df.index, y=od_results_df.od_score_mean, yerr=od_results_df.od_score_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "red_square = dict(markerfacecolor='r', marker='s')\n",
    "plt.boxplot([scores for scores in od_results_df.od_scores.values], flierprops=red_square)\n",
    "plt.violinplot([scores for scores in od_results_df.od_scores.values]);\n",
    "plt.xticks(range(1, len(od_results_df)+1), od_results_df.index); plt.ylabel(\"Mahalanobis distance\");\n",
    "# plt.axhline(od.threshold, c='k', ls='--', label='threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for subset, scores in zip(od_results_df.index, od_results_df.od_scores):\n",
    "    sns.histplot(scores, ax=ax, label=subset, alpha=0.8, color=(random(), random(), random()), bins=100, stat=\"density\")\n",
    "    \n",
    "# plt.xlim([0, 30])\n",
    "plt.axvline(od.threshold, c='k', ls='--', label='Outlier threshold')\n",
    "plt.legend(loc=\"upper right\"); plt.xlabel(\"Mahalanobis distance\"); "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (base)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
